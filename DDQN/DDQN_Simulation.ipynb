{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from DDQN_model_new import Environment, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDQN_Simulation(agent, env, test_model, size , loads):\n",
    "    total_reward = 0\n",
    "    if size == 3:\n",
    "        current_state = (0,1,1,[0,0,0,0,0,0,0,0,0])\n",
    "    else:\n",
    "        current_state = (0,1,1,[0,0,0,0])\n",
    "        \n",
    "    trailer_loc, day_of_week, trailer_size, load_vector = current_state # initial state\n",
    "\n",
    "    for i in range(1000):\n",
    "\n",
    "    #     print('initial state:',current_state)\n",
    "\n",
    "        state_flatten = [] # flatten states\n",
    "        for j in range(3):\n",
    "            state_flatten.append(current_state[j])\n",
    "        for k in range((size*size)):\n",
    "            state_flatten.append(current_state[3][k])\n",
    "\n",
    "        act_values = test_model.predict(np.array(state_flatten).reshape(1, agent.state_space))\n",
    "        predicted_optimal_action = np.argmax(act_values[0]) # action that will produce the best value\n",
    "    #     print(\"action is\", predicted_optimal_action)\n",
    "\n",
    "        current_state_encode = env.get_numeric_state_nomadic_trucker(trailer_loc, day_of_week, trailer_size, load_vector)\n",
    "        reward = env.get_rewards(current_state_encode, predicted_optimal_action) # get reward from action\n",
    "    #     print(\"reward\", reward)\n",
    "        total_reward += reward\n",
    "\n",
    "\n",
    "        next_truck_location, next_day, next_trailer_type, next_load_vector = env.get_actual_next_grid_state(predicted_optimal_action, current_state_encode, is_test) # go to the next state\n",
    "        next_load_vector = loads[i]\n",
    "\n",
    "        if next_load_vector[next_truck_location] == 1:\n",
    "            next_load_vector[next_truck_location] = 0\n",
    "\n",
    "\n",
    "        next_state = next_truck_location, next_day, next_trailer_type, next_load_vector\n",
    "        current_state = next_state  # get new state\n",
    "        trailer_loc, day_of_week, trailer_size, load_vector = current_state\n",
    "\n",
    "    return(total_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible states\n",
    "s = [3]\n",
    "# s = [2]\n",
    "\n",
    "\n",
    "# possible days\n",
    "d = [1, 3, 7]\n",
    "\n",
    "# possible trailer types\n",
    "t = [1, 3, 3]\n",
    "\n",
    "#Batch_size for 2x2\n",
    "# Batch = [15, 32, 64]\n",
    "\n",
    "#Batch_size for 3x3\n",
    "Batch = [32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 4608 states, batch size 32 : 145756.83520000006\n",
      "Result for 4608 states, batch size 64 : 192799.25245666734\n",
      "Result for 4608 states, batch size 128 : 141288.30304333332\n",
      "Result for 41472 states, batch size 32 : 310187.02425333345\n",
      "Result for 41472 states, batch size 64 : 279915.9701766666\n",
      "Result for 41472 states, batch size 128 : 137974.5368433332\n",
      "Result for 96768 states, batch size 32 : 325291.88068666693\n",
      "Result for 96768 states, batch size 64 : 265722.6751399999\n",
      "Result for 96768 states, batch size 128 : 111703.52587666658\n"
     ]
    }
   ],
   "source": [
    "for size in s:\n",
    "    for days, trailers in zip(d,t):\n",
    "        for batch_size in Batch:\n",
    "            num_actions = size*size\n",
    "            state_inputs = 3 + num_actions\n",
    "            state_space_size = (size*size)*days*trailers*(2**(size*size))\n",
    "            my_agent = Agent(num_actions, state_inputs, batch_size, state_space_size)  # number of actions, number of state inputs (when state is flattened)\n",
    "            my_env = Environment(my_agent, size, days, trailers)\n",
    "            my_test_model = my_agent.build_test_model(state_space_size, batch_size)\n",
    "            is_test = True\n",
    "\n",
    "            loads = pickle.load(open(\"Load_\"+str(size)+\"_\"+str(days)+\".pickle\", \"rb\"))        \n",
    "            Result = DDQN_Simulation(my_agent, my_env, my_test_model, size, loads)\n",
    "            print(\"Result for\",str(state_space_size), \"states, batch size\",batch_size,\":\", Result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
